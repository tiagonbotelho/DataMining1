---
title: "Data Mining I - 1st Project"
author: "Pedro Bel√©m, Rui Fonseca, Tiago Botelho"
date: "December 23, 2016"
output:
  pdf_document:
    fig_height: 6
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(gdata)
library(ggplot2)
library(dplyr)
library(reshape2)
library(tidyr)
library(lubridate)
library(DMwR)
library(rpart.plot)
library(nnet)
library(MASS)
load("./data.RData")
```



## Data Pre processing
We begin by reading the data from the crime.xls file into a data frame
```{r eval = FALSE}
data_path <- "./crime.xls"
info <- read.xls(data_path, sheet=1)
```

Then we removed all the instances with outlier dates. To calculate the outliers we considered a date as the number of days since the beginning of unix time.
```{r eval = FALSE}
remove_outliers <- function(info) {
  n_days <- day(days(ymd(info$Date)))
  return(info[n_days >= quantile(n_days, .25) - 1.5*IQR(n_days) & n_days <= quantile(n_days, .75) + 1.5*IQR(n_days),])
}
```

Then, we convert all the missing values to NA.
```{r eval = FALSE}
na_handler <- function(info) {
  info$Beat[info$Beat == 'UNK'] <- NA
  info <- knnImputation(info, k=3)
  info$BlockRange[info$BlockRange=='UNK'] <- NA
  info$Type[info$Type == '-'] <- NA
  info$Suffix[info$Suffix == '-'] <- NA
  info$Offense.Type[info$Offense.Type == "1"] <- NA
  info$Beat <- as.character(info$Beat)
  
  return(info)
}
```



We create new collumns to better explain the data:

* WeekDay: the day of the week when the crime happened
* DayInterval: the interval of the day in which the crime happened, following this criteria:
    1. "Morning" represented as "1", is from the hour interval 8 >= h > 12
    2. "Afternoon" represented as "2", is from the hour interval 12 >= h > 19
    3. "Night" represented as "3", is from the hour interval 19 >= h <= 23 of the same day, plus the hour interval 0 >= h > 8 of the following day
* Year: the year in which the crime happened
* Month: the month in which the crime occoured
* Day: the day of the month in which the crime happened


```{r echo = FALSE, eval = FALSE}
get_days_between <- function(info) {
  firstdate <- info$Date[order(format(as.Date(info$Date)))[1]]
  lastdate <- info$Date[tail(order(format(as.Date(info$Date))), n=1)] 
  
  return(seq(as.Date(firstdate), as.Date(lastdate), by="days"))
}
```

## Data Visualisation

The map in Figure 1. is from the houston police department. As we can see, each police beat is linked to a specific area of the city

![Houston police beat map](./beat_map.png)

![Map legend](./beat_map_legend.png)


```{r echo=FALSE, eval=FALSE}
#this is the variable initialization
info <- read.xls(data_path, sheet=1) %>% remove_outliers %>% na_handler
split <- strsplit(as.character(info$BlockRange), "-")
info$BlockRange <- sapply(split, "[", 1)

info.preprocessed <- dataset_prep(info)
info.preprocessed.group <- group_by(info.preprocessed, WeekDay, DayInterval, Beat, Day, Month, Year) %>% summarize(Offenses = sum(Offenses))
info.preprocessed.total_perm <- create_total_perm(info.preprocessed)
info.preprocessed.joined <- merge(x=info.preprocessed.total_perm, y=info.preprocessed.group, all=TRUE)


info.preprocessed.onlyweek <- dataset_prep(info, only.week = TRUE)
info.preprocessed.onlyweek.group <- group_by(info.preprocessed.onlyweek, WeekDay, DayInterval, Beat) %>% summarize(Offenses = sum(Offenses))
info.preprocessed.onlyweek.total_perm <- create_total_perm(info.preprocessed, only.week = TRUE)
```

Creating graphics with the data, we can begin to find some usefull patterns in the data.

```{r echo=FALSE}

#number of crimes per beat with the types of crimes
info.df <- tbl_df(info) %>% drop_na(Beat, BlockRange)
by_beat <- group_by(info.df, Beat)
crime_count <- arrange(tally(by_beat), desc(n))
# we only want the top ones
top_beats <- head(crime_count)
top_beat_crimes <- filter(info.df,info.df$Beat %in% top_beats$Beat)
ggplot(top_beat_crimes, aes(x = Beat, fill=BlockRange)) + geom_bar(stat="count") + ggtitle("Distribution of crimes per beat")

```
The graph shows the six beats with more crime count. Each beat has continuous block ranges, which makes sense since the block ranges are numbered continuously.



```{r echo=FALSE}
#number of crimes per Address (block range, streetname, type, suffix)
info.df <- tbl_df(info)
by_address <- group_by(info.df,BlockRange, StreetName, Type, Suffix)
count <- arrange(tally(by_address), desc(n))
#criar a morada com as outras colunas coladas
count$d <- paste(count$BlockRange, count$StreetName,count$Type,count$Suffix)
#remover os NA's depois, antes tava dificil
count$d <- gsub(" NA","",count$d)
count$d <- gsub("NA ","",count$d)
#reorder porque o x ficava ordenado por ordem alfabetica e quero pelo n, e -n para ficar por ordem descendente.
ggplot(head(count), aes(x=reorder(d,-n), y=n)) + geom_bar(stat="identity") + ggtitle("Distribution of crimes per Address")
```
We define the base address of a crime as the combination of BlockRange, Streetname, Type and Suffix.
The address with more crime count is 2300 WAYSIDE S.

```{r echo=FALSE}
#number of crimes per street name, Type and Suffix
info.df <- tbl_df(info)
by_street_type_suffix <- group_by(info.df, StreetName, Type, Suffix)
count <- arrange(tally(by_street_type_suffix), desc(n))
#criar a morada com as outras colunas coladas
count$d <- paste(count$StreetName,count$Type,count$Suffix)
#remover os NA's depois, antes tava dificil
count$d <- gsub(" NA","",count$d)
#reorder porque o x ficava ordenado por ordem alfabetica e quero pelo n, e -n para ficar por ordem descendente.
ggplot(head(count), aes(x=reorder(d,-n), y=n)) + geom_bar(stat="identity") + ggtitle("Distribution of crimes per Address except BlockRange")
```
Ignoring the block range in the adress, the adress with more offenses is WESTHEIMER RD.


```{r echo=FALSE}
#number of crimes per street name
info.df <- tbl_df(info)
by_street <- group_by(info.df, StreetName)
count <- arrange(tally(by_street), desc(n))
#reorder porque o x ficava ordenado por ordem alfabetica e quero pelo n, e -n para ficar por ordem descendente.
ggplot(head(count), aes(x=reorder(StreetName,-n), y=n)) + geom_bar(stat="identity") + ggtitle("Distribution of crimes per StreetName")
```
The streetname with more offenses is Westhimer.

```{r echo = FALSE}
#number of crimes per hour and type
ggplot(info, aes(x=Hour, color=Offense.Type)) + geom_histogram(binwidth = 1) + ggtitle("Distribution of crimes per hour and type")

```
We can see that the crimes are most likely to happen during the afternoon.




```{r echo=FALSE}
#number of crimes per hour and type
ggplot(info, aes(x=Hour)) + geom_histogram(binwidth = 1) + facet_wrap(~ Offense.Type) + ggtitle("Distribution of crimes per hour and type")

```
Here we can analyse the hours for each crime type. Theft is the most likely crime, with it's peak at hour 15.

```{r echo=FALSE}
offenses_by_weekday <- group_by(info.preprocessed, WeekDay) %>% summarise(Offenses = sum(Offenses))
ggplot(offenses_by_weekday, aes(x=WeekDay, y=Offenses)) + geom_bar(stat="identity")
```
The day of the week with more offenses is day 5.

##Data Prediction

The question that we're going to answer is: how many offenses will occur, in a given day interval of a date, in a certain police beat.
We found that some columns of the original data aren't necessary for our predictive model. 
Only some information for this problem is actually needed so we had to adapt the given data for this problem. First we noticed that the premise of the crimes are irrelevant and that the only necessary attribute to identify an area is the beat. 

The first direction that we took to answer the problem was to create a new data frame with the follwing collunms:

* WeekDay: the day of the week when the crime happene
* Beat: the police beat
* DayInterval: the interval of the day in which the crime happened
* Offenses: The total number of offenses

We will use this data to construct a model that predicts the Offenses number. We train the model with 70 percent of the data and test with the remaining 30 percent.

###Regression Tree Model


```{r}
######## Regression Trees ##########
sp <- sample(1:nrow(info.preprocessed.onlyweek.joined), as.integer(nrow(info.preprocessed.onlyweek.joined)*0.80))
tr <- info.preprocessed.onlyweek.joined[sp,]
ts <- info.preprocessed.onlyweek.joined[-sp,]
ac <- rpartXse(Offenses ~ ., tr)
ac$xlevels[["y"]] <- union(ac$xlevels[["y"]], levels(ts$Offenses))
ps <- predict(ac, ts, type="vector")
#root mean squared error = 0.29
rmse = sqrt(mean((ts$Offenses-ps)^2))
#mean absolute error = 0.14
mae <- mean(abs(ps - ts$Offenses))
#correlation between the predictions and the true values = 0.60
cr <- cor(ps, ts$Offenses)
prp(ac, type=1, extra=101)
######## Regression Trees ##########
```

We get:

- Root Mean Squared Error = 0.29
- Mean Absolute Error = 0.14
- Correlation between the predictions and the true values = 0.60



###Linear Discriminant Analysis
```{r eval=FALSE}
######## Linear Discriminant Analysis ##########
sp <- sample(1:nrow(info.preprocessed.onlyweek.joined), as.integer(nrow(info.preprocessed.onlyweek.joined)*0.99))
tr <- info.preprocessed.onlyweek.joined[sp,]
ts <- info.preprocessed.onlyweek.joined[-sp,]
ac <- lda(Offenses ~ ., tr)
ps <- predict(ac, ts)
#root mean squared error = N.a.N.
rmse = sqrt(mean((ts$Offenses-as.numeric(levels(ps$class)[ps$class])^2)))
#mean absolute error = 0.248
mae <- mean(abs(as.numeric(levels(ps$class)[ps$class]) - ts$Offenses))
#correlation between the predictions and the true values = 0.42
cr <- cor(as.numeric(levels(ps$class)[ps$class]), ts$Offenses)
######## Linear Discriminant Analysis ##########
```

We get:

- Mean Absolute Error = 0.248
- Correlation between the predictions and the true values = 0.42

###Multiple Linear Regression
```{r}
######## Multiple Linear Regression ##########
sp <- sample(1:nrow(info.preprocessed.onlyweek.joined), as.integer(nrow(info.preprocessed.onlyweek.joined)*0.99))
tr <- info.preprocessed.onlyweek.joined[sp,]
ts <- info.preprocessed.onlyweek.joined[-sp,]
lin.reg <- lm(Offenses ~ ., tr)
final.lin.reg <- step(lin.reg)
ps <- predict(final.lin.reg)
regr.eval(ts$Offenses, ps)
######## Multiple Linear Regression ##########
```

###Neural network

```{r eval=FALSE}
training_index <- sample(1:nrow(info.preprocessed.onlyweek.joined),as.integer(0.70*nrow(info.preprocessed.onlyweek.joined)))
train <- info.preprocessed.onlyweek.joined[training_index,]
test <- info.preprocessed.onlyweek.joined[-training_index,]

max.offenses <- max(info.preprocessed.onlyweek.joined$Offenses)
nn <- nnet(Offenses / max.offenses ~ ., data = train, size=5, decay=0.05, maxit=1000)
pred <- predict(nn, test) * max.offenses
mse <- mean((pred - test$Offenses)^2)
rmse = sqrt(mean((test$Offenses-pred)^2))
mae <- mean(abs(pred - test$Offenses))
cr <- cor(pred, test$Offenses)

```
We get:

- Root Mean Squared Error = 0.28
- Mean Absolute Error = 0.17
- Correlation between the predictions and the true values = 0.54

-----------------------------------

We soon realised that this model was inaccurate because we can only predict the average number of offenses of a weekday. This model would only be useful if we didn`t have data for a certain weekday.

We had to add the date to our data model so we could predict the number of offenses for a date.

So our final data model for this problem consists of

DayInterval | Beat | Day | Month | Year | Offenses
--------------------------------------------------

To convert the given data to this data model, we start by adding the column with the correspondent day interval, and then we create a new data frame only with those column. As there are multiple crimes in the same day interval but we want the number of offenses of the day interval we sum the offenses on the same interval. 

```{r}

dataset_prep <- function(x) {
  x$Date <- ifelse(as.integer(x$Hour) < 8,
                   as.character(as.Date(x$Date) - 1), 
                   as.character(x$Date))
  
  # Split info in time intervals
  x$DayInterval <- 0
  x[as.integer(x$Hour) < 8 | as.integer(x$Hour) >= 19,]$DayInterval <- 3
  x[as.integer(x$Hour) >= 12 & as.integer(x$Hour) < 19,]$DayInterval <- 2
  x[as.integer(x$Hour) >= 8 & as.integer(x$Hour) < 12,]$DayInterval <- 1

  ret <- data.frame(WeekDay = as.integer(strftime(x$Date, "%u")),
                    DayInterval = x$DayInterval,
                    Beat = x$Beat,
                    Offenses = x$X..offenses,
                    Day = day(x$Date),
                    Month = month(x$Date),
                    Year = year(x$Date),
                    stringsAsFactors = FALSE)
  ret <- group_by(ret, WeekDay, DayInterval, Beat, Day, Month, Year) %>% 
    summarize(Offenses = sum(Offenses))
  
  return(ret)
}

```

At this moment we have the given data adjusted to the problem but we only have cases where was crimes, so we needed to generate all combinations with 0 offenses. 

So we generate a new data frame with all combinations of DayInterval, Beat, Day, Month, Year and added to our data only the combinations missing, with Offenses as 0. To do that we merged the 2 data frames and removed the rows with duplicated combination (DayInterval, Beat, Day, Month, Year) and 0 Offenses. 
```{r eval=FALSE}
get_days_between <- function(info) {
  firstdate <- info$Date[order(format(as.Date(info$Date)))[1]]
  lastdate <- info$Date[tail(order(format(as.Date(info$Date))), n=1)] 
  
  return(seq(as.Date(firstdate), as.Date(lastdate), by="days"))
}

create_total_perm <- function(preprocessed) {
  days.between <- get_days_between(info)
  unique_beats <- unique(preprocessed$Beat)
  unique_day_intervals <- unique(preprocessed$DayInterval)
  
  all_beats_perm <- data.frame(Date = rep(days.between, 
                                          times=length(unique_beats) * 
                                            length(unique_day_intervals)),
                               DayInterval = rep(unique_day_intervals, 
                                                 times=length(unique_beats) * 
                                                   length(days.between)),
                               Beat = rep(unique_beats, 
                                          times=length(unique_day_intervals) * 
                                            length(days.between)),
                               Offenses = rep(0, 
                                              times=length(unique_day_intervals) * 
                                                length(days.between) * 
                                                length(unique_beats)))
  all_beats_perm$DayInterval <- as.integer(all_beats_perm$DayInterval)
  all_beats_perm$Offenses <- as.character(all_beats_perm$Offenses)
  all_beats_perm$WeekDay = as.integer(strftime(all_beats_perm$Date, "%u"))  
  all_beats_perm$Day <- day(as.Date(all_beats_perm$Date))
  all_beats_perm$Month <- month(as.Date(all_beats_perm$Date))
  all_beats_perm$Year <- year(as.Date(all_beats_perm$Date))
  all_beats_perm <- all_beats_perm[,!colnames(all_beats_perm) %in% c("Date")]
  
  return(all_beats_perm)
}

info.preprocessed.total_perm <- create_total_perm(info.preprocessed)
info.preprocessed.joined <- merge(info.preprocessed, info.preprocessed.total_perm, 
                                  by=c("WeekDay", "DayInterval", "Beat", "Day", "Month", "Year"),
                                  all=TRUE)
info.preprocessed.joined[is.na(info.preprocessed.joined$Offenses.x),]$Offenses.x <- 0
info.preprocessed.joined$Offenses <- info.preprocessed.joined$Offenses.x
info.preprocessed.joined <- info.preprocessed.joined[,!colnames(info.preprocessed.joined) %in%
                                                       c("Offenses.y", "Offenses.x")]

```
We considered that the given data had all occorrences between the first and the last date of the set.

###Using a neural network

```{r eval=FALSE}
training_index <- sample(1:nrow(info.preprocessed.joined),as.integer(0.70*nrow(info.preprocessed.joined)))
train <- info.preprocessed.joined[training_index,]
test <- info.preprocessed.joined[-training_index,]

max.offenses <- max(info.preprocessed.joined$Offenses)
nn <- nnet(Offenses / max.offenses ~ ., data = train, size=5, decay=0.05, maxit=1000)
pred <- predict(nn, test) * max.offenses
mse <- mean((pred - test$Offenses)^2)
rmse = sqrt(mean((test$Offenses-pred)^2))
mae <- mean(abs(pred - test$Offenses))
cr <- cor(pred, test$Offenses)

```
We get:

- Root Mean Squared Error = 0.29
- Mean Absolute Error = 0.18
- Correlation between the predictions and the true values = 0.62

###Using a regression tree
```{r eval=FALSE}
training_index <- sample(1:nrow(info.preprocessed.joined),as.integer(0.70*nrow(info.preprocessed.joined)))
train <- info.preprocessed.joined[training_index,]
test <- info.preprocessed.joined[-training_index,]
ac <- rpartXse(Offenses ~ ., train)
ps <- predict(ac, test, type="vector")
#root mean squared error = 0.29
rmse = sqrt(mean((test$Offenses-ps)^2))
#mean absolute error = 1.89
mae <- mean(abs(ps - test$Offenses))
#correlation between the predictions and the true values = 0.60
cr <- cor(ps, test$Offenses)
prp(ac, type=1, extra=101)
```


We get:

- Root Mean Squared Error = 0.85
- Mean Absolute Error = 0.53
- Correlation between the predictions and the true values = 0.59

###Using a Linear Discriminant Analysis
```{r eval=FALSE}
sp <- sample(1:nrow(info.preprocessed.joined), as.integer(nrow(info.preprocessed.joined)*0.70))
tr <- info.preprocessed.onlyweek.joined[sp,]
ts <- info.preprocessed.onlyweek.joined[-sp,]
ac <- lda(Offenses ~ ., tr)
ps <- predict(ac, ts)
#mean absolute error = 0.248
mae <- mean(abs(as.numeric(levels(ps$class)[ps$class]) - ts$Offenses))
#correlation between the predictions and the true values = 0.42
cr <- cor(as.numeric(levels(ps$class)[ps$class]), ts$Offenses)
```
We get:

- Mean Absolute Error = 0.21
- Correlation between the predictions and the true values = 0.48
